# -*- coding: utf-8 -*-
"""Copy of NFT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1m97n6YEEdlT_wAdCjKAAAuShgSyIB2Cu
"""


"""# New Section"""

!wget https://raw.githubusercontent.com/Rainisha-varma/NFT/refs/heads/main/Book1.csv?token=GHSAT0AAAAAAC2Z2GAEPPXS6CDWYPHVE27CZZ677SQ -O Dataset.csv


import pandas as pd
from scipy.stats import zscore

# Load your dataset (assuming it's in a CSV format, adjust accordingly)
df = pd.read_csv('Dataset.csv')

# Step 1: Organize the data for easier calculations
df_melt = df.melt(id_vars=['Commodity', 'Year'],
                  value_vars=['Jan', 'Feb', 'Mar', 'April', 'May', 'Jun', 'July', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'],
                  var_name='Month', value_name='Price')


# Step 2: Calculate Z-score by grouping data by 'Commodity' and applying zscore function
df_melt['Z-score'] = df_melt.groupby('Commodity')['Price'].transform(zscore)

# Step 3: Drop the 'Price' column
zonly = df_melt.drop(columns=['Price'])

# Step 4: Save the result to a new CSV file
zonly.to_csv('normalized_data.csv', index=False)

# Print confirmation message
#print("Z-scores have been saved to 'normalized_data.csv'")


# Identify outliers where Z-score is beyond a threshold (e.g., >3 or <-3)
outliers = df_melt[(df_melt['Z-score'] > 3) | (df_melt['Z-score'] < -3)]
print(outliers)

# Cap Z-scores beyond 3 or -3
df_melt['Z-score'] = df_melt['Z-score'].apply(lambda x: 3 if x > 3 else (-3 if x < -3 else x))

# Identify outliers where Z-score is beyond a threshold (e.g., >3 or <-3)
outliers = df_melt[(df_melt['Z-score'] > 3) | (df_melt['Z-score'] < -3)]
print(outliers)

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

# Step 4: One-hot encode categorical columns ('Commodity', 'Month')
df_melt_encoded = pd.get_dummies(df_melt, columns=['Commodity', 'Month'], drop_first=True)

# Step 5: Use Z-scores as features (X) and Price as target (y)
X = df_melt_encoded.drop(columns=['Price', 'Z-score'])  # Use Z-score instead of raw prices
X['Z-score'] = df_melt['Z-score']  # Include Z-score as a feature
Y = df_melt_encoded['Price']  # Target remains the actual price

#X['Year'] = df_melt['Year']

# Step 6: Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

print(df_melt_encoded.columns)

print(X.columns)

print(Y.head)

# Step 7: Train the Linear Regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Step 8: Predict on the test set
y_pred = model.predict(X_test)

import numpy as np

# Step 1: Make predictions on the training set
y_train_pred = model.predict(X_train)

# Step 2: Calculate Mean Squared Error (MSE) for the training set
mse_train = mean_squared_error(y_train, y_train_pred)

# Step 3: Calculate Root Mean Squared Error (RMSE) for the training set
rmse_train = np.sqrt(mse_train)

# Step 4: Print the RMSE
print(f"Training RMSE: {rmse_train}")

# Step 9: Evaluate the model
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
print(f"RMSE: {rmse}")

import pandas as pd

# Step 1: Prepare your input features
# Replace these with actual values you want to predict
new_data = {
    'Month_April': [1],  # Example: April = 1, other months = 0
    #'Month_Aug': [0],    # Example: Aug = 0
    #'Month_Dec': [0],    # Example: Dec = 0
    #'Month_Feb': [0],    # Example: Feb = 0
    #'Month_Jan': [0],    # Example: Jan = 0
    #'Month_July': [0],   # Example: July = 0
    #'Month_Jun': [0],    # Example: Jun = 0
    #'Month_Mar': [0],    # Example: Mar = 0
    #'Month_May': [0],    # Example: May = 0
    #'Month_Nov': [0],    # Example: Nov = 0
    #'Month_Oct': [0],    # Example: Oct = 0
    #'Month_Sep': [0],    # Example: Sep = 0
    #'Commodity_Masoor Dal': [0],  # Example: Commodity_X = 1 if it's a relevant commodity, otherwise 0
    'Commodity_Groundnut Oil': [1],  # Example: Commodity_Y = 0
    # Add or remove commodity columns as necessary
    'Year': [2025]       # Example year
}

# Step 2: Create a DataFrame
new_df = pd.DataFrame(new_data)

# Ensure the DataFrame has the same columns as the training data
# For example, you may need to reorder columns or ensure all columns are present
for col in X.columns:
    if col not in new_df.columns:
        new_df[col] = 0  # Or appropriate default value
new_df = new_df[X.columns]  # Reorder columns if necessary

# Step 3: Get predictions
y_new_pred = model.predict(new_df)

print(f"Predicted Price: {y_new_pred[0]}")

print(df_melt_encoded.columns)

import matplotlib.pyplot as plt

# Step 1: Plot the Actual vs Predicted values
plt.figure(figsize=(10,6))
plt.scatter(y_test, y_pred, color='blue', label='Predicted Prices')

# Step 2: Plot the line where the prediction perfectly matches the actual prices
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', lw=2, label='Perfect Prediction Line')

# Step 3: Add labels and title
plt.xlabel('Actual Prices')
plt.ylabel('Predicted Prices')
plt.title('Actual vs Predicted Prices (Linear Regression)')
plt.legend()
plt.grid(True)

# Step 4: Show the plot
plt.show()

from google.colab import drive
drive.mount('/content/drive')

from sklearn.metrics import r2_score

# Assuming y_test and y_pred are your actual and predicted values, respectively.
r2 = r2_score(y_test, y_pred)
print(f"R-squared (R2): {r2}")